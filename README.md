

## Otimiza√ß√£o do Spark

Aqui est√£o algumas dicas para otimizar o Spark e economizar recursos:

O "bot√£o" que 90% dos usu√°rios Spark usa errado: spark.sql.shuffle.partitions.

(Salve ‚ôªÔ∏è porque o default "200" est√° custando caro para o seu cluster).

Quando o Spark faz um shuffle (um join, groupBy ou sort), ele precisa decidir em quantos "peda√ßos" (parti√ß√µes) ele vai quebrar o resultado.

Esse n√∫mero √© controlado pelo spark.sql.shuffle.partitions.

O valor padr√£o? 200.

E aqui mora o problema.

"200" √© um chute. √â um n√∫mero gen√©rico que n√£o faz ideia se voc√™ est√° processando 10MB ou 10TB.
<p style="text-align: justify;">
    Cen√°rio A: "Small Data" (Ex: 50MB)
    Voc√™ faz um groupBy. O Spark, obediente, cria 200 parti√ß√µes.
    Resultado: 195 parti√ß√µes vazias.
</p>

Voc√™ gastou overhead de CPU e agendador para orquestrar 200 tarefas quando 5 seriam suficientes

### 1. Otimize o Shuffle Partitions
O par√¢metro `spark.sql.shuffle.partitions` controla o n√∫mero de parti√ß√µes usadas durante as trocas de dados (joins e agrega√ß√µes). A recomenda√ß√£o √© manter cada parti√ß√£o entre 100 MB e 200 MB.

| Tamanho Total dos Dados | `spark.sql.shuffle.partitions` | Justificativa |
| :--- | :--- | :--- |
| **Pequeno** (< 1 GB) | 10 a 50 | Evita o overhead de muitas tarefas pequenas. |
| **M√©dio** (1 GB a 10 GB) | 50 a 200 | Mant√©m o paralelismo alinhado com executores m√©dios. |
| **Grande** (10 GB a 100 GB) | 200 a 1000 | Evita sobrecarga no Garbage Collection (GC). |
| **Muito Grande** (> 100 GB) | 1000+ | Necess√°rio para distribuir carga em clusters massivos. |

Rela√ß√£o com o maxPartitionBytes

Enquanto o spark.sql.shuffle.partitions controla os dados durante as trocas (joins/agregados), o seu guia menciona o spark.sql.files.maxPartitionBytes. Este √∫ltimo controla a leitura inicial do disco.

<p style="text-align: justify;">
    Se voc√™ ler 10 GB de dados com maxPartitionBytes em 128 MB, ter√° inicialmente cerca de 80 parti√ß√µes.
    Se voc√™ n√£o ajustar o shuffle, o Spark usar√° o padr√£o de 200, o que pode ser excessivo para esse volume, gerando tarefas vazias.
</p>

Dica Extra: Sempre monitore a aba SQL no Spark UI. Se voc√™ notar que o "Shuffle Read Size" por tarefa est√° muito alto (ex: > 500 MB), aumente o n√∫mero de parti√ß√µes para evitar o uso excessivo de mem√≥ria do executor (spark.executor.memory).

> **Dica:** No Spark 3.0+, habilite o AQE (`spark.sql.adaptive.enabled`) para que o Spark ajuste esse n√∫mero automaticamente.

### 2. Ajuste o tamanho dos blocos (block size)
O par√¢metro `spark.sql.files.maxPartitionBytes` define o tamanho m√°ximo dos blocos lidos do disco, ajudando a reduzir o n√∫mero de tarefas iniciais.

| Tamanho do arquivo | spark.sql.files.maxPartitionBytes | spark.sql.files.openCostInBytes |
| :--- | :--- | :--- |
| **Pequeno** (< 100 MB) | 32 MB a 64 MB | 1 MB a 4 MB |
| **M√©dio** (100 MB a 1 GB) | 64 MB a 128 MB | 4 MB a 16 MB |
| **Grande** (1 GB a 10 GB) | 128 MB a 256 MB | 16 MB a 64 MB |
| **Muito grande** (> 10 GB) | 256 MB a 512 MB | 64 MB a 128 MB |

* **Regra geral:** O tamanho dos blocos deve ser entre 1/10 e 1/5 do tamanho do arquivo.
* **Custo de abertura:** Deve ser entre 1/100 e 1/50 do tamanho do bloco.


### 3. Use o cache de dados
* Use `spark.cache` para armazenar dados acessados frequentemente em mem√≥ria.
* Utilize `cache()` ou `persist()` para evitar reprocessamento e reduzir leitura de disco.

### 4. Otimize as jun√ß√µes (joins)
O **broadcast** envia tabelas pequenas para todos os n√≥s, permitindo jun√ß√µes locais sem shuffle.

| Categoria da Tabela | Tamanho | Ajuste do `spark.sql.autoBroadcastJoinThreshold` |
| :--- | :--- | :--- |
| **Pequena** | < 10 MB | Transmitida automaticamente (padr√£o). |
| **M√©dia** | 10 MB a 100 MB | Aumente para 50 MB ou 100 MB. |
| **Grande** | > 100 MB | Geralmente n√£o √© transmitida automaticamente. |



### 5. Monitore e ajuste do paralelismo e garbage collection (GC)
Ajuste o `spark.default.parallelism` e a mem√≥ria do executor para evitar falhas e lentid√£o no processamento.

| Tamanho dos dados | spark.default.parallelism | spark.executor.memory |
| :--- | :--- | :--- |
| **Pequeno** (< 100 MB) | 2-4 | 1-2 GB |
| **M√©dio** (100 MB a 1 GB) | 4-8 | 2-4 GB |
| **Grande** (1 GB a 10 GB) | 8-16 | 4-8 GB |
| **Muito grande** (> 10 GB) | 16-32 | 8-16 GB |


*Tamanho dos dados pequeno (< 100 MB)*

- `spark.default.parallelism`: 2-4
- `spark.sql.files.openCostInBytes`: 1-4 MB

Exemplo:
spark.conf.set("spark.default.parallelism", 2)

spark.conf.set("spark.sql.files.openCostInBytes", 1 * 1024 * 1024) # 1 MB


*Tamanho dos dados m√©dio (100 MB a 1 GB)*

- `spark.default.parallelism`: 4-8
- `spark.sql.files.openCostInBytes`: 4-16 MB

Exemplo:
spark.conf.set("spark.default.parallelism", 4)

spark.conf.set("spark.sql.files.openCostInBytes", 4 * 1024 * 1024) # 4 MB


*Tamanho dos dados grande (1 GB a 10 GB)*

- `spark.default.parallelism`: 8-16
- `spark.sql.files.openCostInBytes`: 16-64 MB

Exemplo:
spark.conf.set("spark.default.parallelism", 8)

spark.conf.set("spark.sql.files.openCostInBytes", 16 * 1024 * 1024) # 16 MB


*Tamanho dos dados muito grande (> 10 GB)*

- `spark.default.parallelism`: 16-32
- `spark.sql.files.openCostInBytes`: 64-128 MB

Exemplo:
spark.conf.set("spark.default.parallelism", 16)

spark.conf.set("spark.sql.files.openCostInBytes", 64 * 1024 * 1024) # 64 MB

Lembre-se de que esses s√£o apenas exemplos e que o ajuste desses par√¢metros depende do seu ambiente de execu√ß√£o e do tamanho dos dados.

*Regra geral*

- `spark.default.parallelism`: 2-4 vezes o n√∫mero de n√∫cleos de CPU dispon√≠veis.
- `spark.sql.files.openCostInBytes`: 1-10% do tamanho do arquivo.



H√° v√°rias configura√ß√µes de mem√≥ria RAM do executor que voc√™ pode ajustar no Spark:

1. spark.executor.memory: define a mem√≥ria RAM total dispon√≠vel para cada executor
2. spark.executor.memoryOverhead: define a mem√≥ria adicional para o executor (por exemplo, para o sistema operacional e outros processos)
3. spark.memory.fraction: define a fra√ß√£o de mem√≥ria RAM usada para armazenamento de dados (padr√£o: 0,6)
4. spark.memory.storageFraction: define a fra√ß√£o de mem√≥ria RAM usada para armazenamento de dados em cache (padr√£o: 0,5)
5. spark.executor.pyspark.memory: define a mem√≥ria RAM dispon√≠vel para o Python worker (somente para PySpark)
6. spark.executor.pyspark.memoryOverhead: define a mem√≥ria adicional para o Python worker (somente para PySpark)

Exemplo:
*  spark.conf.set("spark.executor.memory", "4g") - 4 GB de mem√≥ria RAM
*  spark.conf.set("spark.executor.memoryOverhead", "1g") - 1 GB de mem√≥ria adicional
*  spark.conf.set("spark.memory.fraction", 0.6) - 60% da mem√≥ria RAM para armazenamento de dados
*  spark.conf.set("spark.memory.storageFraction", 0.5) - 50% da mem√≥ria RAM para armazenamento de dados em cache

Lembre-se de que o ajuste dessas configura√ß√µes depende do seu ambiente de execu√ß√£o e do tamanho dos dados.



#### Configura√ß√µes de RAM do Executor:
* `spark.executor.memoryOverhead`: Mem√≥ria para o SO e processos externos.
* `spark.memory.fraction`: Fra√ß√£o da RAM para armazenamento (padr√£o 0.6).
* `spark.memory.storageFraction`: Fra√ß√£o da RAM para cache (padr√£o 0.5).

Monitoramento do GC

    1. Acesse o Spark UI em `http://<driver-node>:4040`
    2. Clique em "Executors"
    3. Verifique a coluna "GC Time" para cada executor
    4. Se o tempo de GC for alto (> 10%), ajuste a mem√≥ria do executor

### 6. Use o Spark SQL
O Spark SQL (DataFrames e Datasets) √© mais eficiente que a RDD API devido ao otimizador Catalyst.


Lembre-se de monitorar o desempenho do seu aplicativo Spark e ajustar as configura√ß√µes conforme necess√°rio! üòä

## Python

1. Estruturas de Dados: Tuplas vs. Listas
As tuplas s√£o imut√°veis e possuem um tamanho fixo, o que torna sua aloca√ß√£o de mem√≥ria muito mais r√°pida que a das listas, que precisam de espa√ßo extra para redimensionamento din√¢mico.

    Exemplo:
    Python
    Lento: Lista (mut√°vel)
    minha_lista = [1, 2, 3, 4, 5] 
    
    R√°pido: Tupla (imut√°vel)
    minha_tupla = (1, 2, 3, 4, 5) 


    Resultado: Em testes, a cria√ß√£o de uma tupla pode ser cerca de 6 vezes mais r√°pida que a de uma lista2.


2. Buscas com Sets e Dicion√°rios
Dicion√°rios e conjuntos (sets) utilizam tabelas de hash, permitindo que o Python encontre um item diretamente sem percorrer toda a estrutura3333. Isso resulta em uma busca de tempo constante, denotada como $O(1)$4.

    Exemplo:
    Python
    Lento em listas grandes: O Python olha item por item
    if 999999 in lista_de_um_milhao: 
        pass
    
    Instant√¢neo em Sets/Dicts: O Python vai direto ao endere√ßo
    if 999999 in set_de_um_milhao: 
        pass


    Performance: Enquanto a busca em uma lista grande pode levar milissegundos, em um set ou dicion√°rio o tempo √© virtualmente zero5.


3. Vari√°veis Locais vs. Globais
O Python utiliza a regra LEGB para buscar vari√°veis, come√ßando sempre pelo escopo local6666. Como o escopo local √© menor, a busca √© muito mais √°gil do que no escopo global.

    Exemplo:
    Python
    Menos eficiente
    contador_global = 0
    def teste_global():
        global contador_global
        for i in range(1000000):
            contador_global += 1
    
    Mais eficiente
    def teste_local():
        contador_local = 0
        for i in range(1000000):
            contador_local += 1


    Nota: O uso de vari√°veis locais pode reduzir o tempo de execu√ß√£o em cerca de 35% em loops intensivos8.


4. Encapsulamento em Classes
Manter vari√°veis restritas a fun√ß√µes e classes ajuda o interpretador a gerenciar menos nomes simultaneamente, melhorando a performance e a gest√£o de mem√≥ria.

    Exemplo:
    Python
    class RetanguloEncapsulado:
        def __init__(self, largura, altura):
            self._largura = largura # Atributo protegido
            self._altura = altura
    
        def area(self):
            return self._largura * self._altura


    Benef√≠cio: Al√©m da performance, evita conflitos de nomes e garante que os dados n√£o sejam modificados acidentalmente por c√≥digo externo.


5. List Comprehensions e Geradores
As compreens√µes de lista s√£o otimizadas internamente, sendo mais r√°pidas que o uso do m√©todo .append() dentro de um loop for tradicional.

    Exemplo de List Comprehension:
    Python
    R√°pido
    quadrados = [x**2 for x in range(10)]


    Exemplo de Express√£o Geradora:
    Python
    Economiza mem√≥ria (n√£o cria a lista inteira de uma vez)
    soma_quadrados = sum(x**2 for x in range(1000000))


    Compara√ß√£o: Express√µes geradoras s√£o mais r√°pidas e consomem muito menos mem√≥ria ao lidar com grandes volumes de dados12.


6. Fun√ß√µes Built-in e NumPy
Sempre prefira as fun√ß√µes nativas do Python (escritas em C) ou bibliotecas especializadas como o NumPy para processamento num√©rico.

    Exemplo (Ordena√ß√£o):
    Python
     Lento: Algoritmo manual (Bubble Sort)
    def bubble_sort(arr): ... 
    
    Instant√¢neo: Fun√ß√£o nativa
    sorted(meu_array)


    Exemplo (Soma com NumPy):
    Python
    import numpy as np
    Muito mais r√°pido que sum() do Python para arrays gigantes
    total = np.sum(array_numpy) 


    Diferen√ßa: Em arrays grandes, o NumPy pode realizar opera√ß√µes em 0.008 segundos, enquanto uma fun√ß√£o Python customizada levaria mais de 1 segundo.




