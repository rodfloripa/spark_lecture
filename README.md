

## Otimiza√ß√£o do Spark

Aqui est√£o algumas dicas para otimizar o Spark e economizar recursos:

O "bot√£o" que 90% dos usu√°rios Spark usa errado: spark.sql.shuffle.partitions.

(Salve ‚ôªÔ∏è porque o default "200" est√° custando caro para o seu cluster).

Quando o Spark faz um shuffle (um join, groupBy ou sort), ele precisa decidir em quantos "peda√ßos" (parti√ß√µes) ele vai quebrar o resultado.

Esse n√∫mero √© controlado pelo spark.sql.shuffle.partitions.

O valor padr√£o? 200.

E aqui mora o problema.

"200" √© um chute. √â um n√∫mero gen√©rico que n√£o faz ideia se voc√™ est√° processando 10MB ou 10TB.
<p style="text-align: justify;">
    Cen√°rio A: "Small Data" (Ex: 50MB)
    Voc√™ faz um groupBy. O Spark, obediente, cria 200 parti√ß√µes.
    Resultado: 195 parti√ß√µes vazias.
</p>

Voc√™ gastou overhead de CPU e agendador para orquestrar 200 tarefas quando 5 seriam suficientes

### 1. Otimize o Shuffle Partitions
O par√¢metro `spark.sql.shuffle.partitions` controla o n√∫mero de parti√ß√µes usadas durante as trocas de dados (joins e agrega√ß√µes). A recomenda√ß√£o √© manter cada parti√ß√£o entre 100 MB e 200 MB.

| Tamanho Total dos Dados | `spark.sql.shuffle.partitions` | Justificativa |
| :--- | :--- | :--- |
| **Pequeno** (< 1 GB) | 10 a 50 | Evita o overhead de muitas tarefas pequenas. |
| **M√©dio** (1 GB a 10 GB) | 50 a 200 | Mant√©m o paralelismo alinhado com executores m√©dios. |
| **Grande** (10 GB a 100 GB) | 200 a 1000 | Evita sobrecarga no Garbage Collection (GC). |
| **Muito Grande** (> 100 GB) | 1000+ | Necess√°rio para distribuir carga em clusters massivos. |

Rela√ß√£o com o maxPartitionBytes

Enquanto o spark.sql.shuffle.partitions controla os dados durante as trocas (joins/agregados), o seu guia menciona o spark.sql.files.maxPartitionBytes. Este √∫ltimo controla a leitura inicial do disco.

<p style="text-align: justify;">
    Se voc√™ ler 10 GB de dados com maxPartitionBytes em 128 MB, ter√° inicialmente cerca de 80 parti√ß√µes.
    Se voc√™ n√£o ajustar o shuffle, o Spark usar√° o padr√£o de 200, o que pode ser excessivo para esse volume, gerando tarefas vazias.
</p>

Dica Extra: Sempre monitore a aba SQL no Spark UI. Se voc√™ notar que o "Shuffle Read Size" por tarefa est√° muito alto (ex: > 500 MB), aumente o n√∫mero de parti√ß√µes para evitar o uso excessivo de mem√≥ria do executor (spark.executor.memory).

> **Dica:** No Spark 3.0+, habilite o AQE (`spark.sql.adaptive.enabled`) para que o Spark ajuste esse n√∫mero automaticamente.

### 2. Ajuste o tamanho dos blocos (block size)
O par√¢metro `spark.sql.files.maxPartitionBytes` define o tamanho m√°ximo dos blocos lidos do disco, ajudando a reduzir o n√∫mero de tarefas iniciais.

| Tamanho do arquivo | spark.sql.files.maxPartitionBytes | spark.sql.files.openCostInBytes |
| :--- | :--- | :--- |
| **Pequeno** (< 100 MB) | 32 MB a 64 MB | 1 MB a 4 MB |
| **M√©dio** (100 MB a 1 GB) | 64 MB a 128 MB | 4 MB a 16 MB |
| **Grande** (1 GB a 10 GB) | 128 MB a 256 MB | 16 MB a 64 MB |
| **Muito grande** (> 10 GB) | 256 MB a 512 MB | 64 MB a 128 MB |

* **Regra geral:** O tamanho dos blocos deve ser entre 1/10 e 1/5 do tamanho do arquivo.
* **Custo de abertura:** Deve ser entre 1/100 e 1/50 do tamanho do bloco.


### 3. Use o cache de dados
* Use `spark.cache` para armazenar dados acessados frequentemente em mem√≥ria.
* Utilize `cache()` ou `persist()` para evitar reprocessamento e reduzir leitura de disco.

### 4. Otimize as jun√ß√µes (joins)
O **broadcast** envia tabelas pequenas para todos os n√≥s, permitindo jun√ß√µes locais sem shuffle.

| Categoria da Tabela | Tamanho | Ajuste do `spark.sql.autoBroadcastJoinThreshold` |
| :--- | :--- | :--- |
| **Pequena** | < 10 MB | Transmitida automaticamente (padr√£o). |
| **M√©dia** | 10 MB a 100 MB | Aumente para 50 MB ou 100 MB. |
| **Grande** | > 100 MB | Geralmente n√£o √© transmitida automaticamente. |



### 5. Monitore e ajuste do paralelismo e garbage collection (GC)
Ajuste o `spark.default.parallelism` e a mem√≥ria do executor para evitar falhas e lentid√£o no processamento.

| Tamanho dos dados | spark.default.parallelism | spark.executor.memory |
| :--- | :--- | :--- |
| **Pequeno** (< 100 MB) | 2-4 | 1-2 GB |
| **M√©dio** (100 MB a 1 GB) | 4-8 | 2-4 GB |
| **Grande** (1 GB a 10 GB) | 8-16 | 4-8 GB |
| **Muito grande** (> 10 GB) | 16-32 | 8-16 GB |


*Tamanho dos dados pequeno (< 100 MB)*

- `spark.default.parallelism`: 2-4
- `spark.sql.files.openCostInBytes`: 1-4 MB

Exemplo:
spark.conf.set("spark.default.parallelism", 2)

spark.conf.set("spark.sql.files.openCostInBytes", 1 * 1024 * 1024) # 1 MB


*Tamanho dos dados m√©dio (100 MB a 1 GB)*

- `spark.default.parallelism`: 4-8
- `spark.sql.files.openCostInBytes`: 4-16 MB

Exemplo:
spark.conf.set("spark.default.parallelism", 4)

spark.conf.set("spark.sql.files.openCostInBytes", 4 * 1024 * 1024) # 4 MB


*Tamanho dos dados grande (1 GB a 10 GB)*

- `spark.default.parallelism`: 8-16
- `spark.sql.files.openCostInBytes`: 16-64 MB

Exemplo:
spark.conf.set("spark.default.parallelism", 8)

spark.conf.set("spark.sql.files.openCostInBytes", 16 * 1024 * 1024) # 16 MB


*Tamanho dos dados muito grande (> 10 GB)*

- `spark.default.parallelism`: 16-32
- `spark.sql.files.openCostInBytes`: 64-128 MB

Exemplo:
spark.conf.set("spark.default.parallelism", 16)

spark.conf.set("spark.sql.files.openCostInBytes", 64 * 1024 * 1024) # 64 MB

Lembre-se de que esses s√£o apenas exemplos e que o ajuste desses par√¢metros depende do seu ambiente de execu√ß√£o e do tamanho dos dados.

*Regra geral*

- `spark.default.parallelism`: 2-4 vezes o n√∫mero de n√∫cleos de CPU dispon√≠veis.
- `spark.sql.files.openCostInBytes`: 1-10% do tamanho do arquivo.



H√° v√°rias configura√ß√µes de mem√≥ria RAM do executor que voc√™ pode ajustar no Spark:

1. spark.executor.memory: define a mem√≥ria RAM total dispon√≠vel para cada executor
2. spark.executor.memoryOverhead: define a mem√≥ria adicional para o executor (por exemplo, para o sistema operacional e outros processos)
3. spark.memory.fraction: define a fra√ß√£o de mem√≥ria RAM usada para armazenamento de dados (padr√£o: 0,6)
4. spark.memory.storageFraction: define a fra√ß√£o de mem√≥ria RAM usada para armazenamento de dados em cache (padr√£o: 0,5)
5. spark.executor.pyspark.memory: define a mem√≥ria RAM dispon√≠vel para o Python worker (somente para PySpark)
6. spark.executor.pyspark.memoryOverhead: define a mem√≥ria adicional para o Python worker (somente para PySpark)

Exemplo:
*  spark.conf.set("spark.executor.memory", "4g") - 4 GB de mem√≥ria RAM
*  spark.conf.set("spark.executor.memoryOverhead", "1g") - 1 GB de mem√≥ria adicional
*  spark.conf.set("spark.memory.fraction", 0.6) - 60% da mem√≥ria RAM para armazenamento de dados
*  spark.conf.set("spark.memory.storageFraction", 0.5) - 50% da mem√≥ria RAM para armazenamento de dados em cache

Lembre-se de que o ajuste dessas configura√ß√µes depende do seu ambiente de execu√ß√£o e do tamanho dos dados.



#### Configura√ß√µes de RAM do Executor:
* `spark.executor.memoryOverhead`: Mem√≥ria para o SO e processos externos.
* `spark.memory.fraction`: Fra√ß√£o da RAM para armazenamento (padr√£o 0.6).
* `spark.memory.storageFraction`: Fra√ß√£o da RAM para cache (padr√£o 0.5).

Monitoramento do GC

    1. Acesse o Spark UI em `http://<driver-node>:4040`
    2. Clique em "Executors"
    3. Verifique a coluna "GC Time" para cada executor
    4. Se o tempo de GC for alto (> 10%), ajuste a mem√≥ria do executor

### 6. Use o Spark SQL
O Spark SQL (DataFrames e Datasets) √© mais eficiente que a RDD API devido ao otimizador Catalyst.

---
Lembre-se de monitorar o desempenho do seu aplicativo Spark e ajustar as configura√ß√µes conforme necess√°rio! üòä
